# train.py
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms, models
from tqdm import tqdm
import numpy as np
import multiprocessing


def main():
    multiprocessing.freeze_support()

    # ===== æ–°å¢ï¼šè¯¦ç»†çš„ GPU æ£€æµ‹æŠ¥å‘Š =====
    print("============== GPU æ£€æµ‹æŠ¥å‘Š ==============")
    print(f"PyTorch version: {torch.__version__}")
    print(f"CUDA available: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"CUDA version: {torch.version.cuda}")
        print(f"GPU device: {torch.cuda.get_device_name(0)}")
        print(f"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
    else:
        print("âš ï¸ æœªæ£€æµ‹åˆ° GPUï¼Œå°†ä½¿ç”¨ CPU è®­ç»ƒï¼ˆé€Ÿåº¦è¾ƒæ…¢ï¼‰")
    print("========================================\n")

    # ===== ä¿æŒåŸæœ‰é…ç½® =====
    DATA_DIR = "data"
    BATCH_SIZE = 32
    NUM_EPOCHS = 15
    LEARNING_RATE = 0.0001
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    print(f"Using device: {DEVICE} | PyTorch version: {torch.__version__}")
    # =============== æ•°æ®é¢„å¤„ç†ï¼ˆé€‚é…ä½ çš„æ•°æ®ç»“æ„ï¼‰===============
    # è®­ç»ƒæ—¶å¢å¼ºï¼šéšæœºç¿»è½¬+è£å‰ª
    transform_train = transforms.Compose([
        transforms.Resize(256),
        transforms.RandomResizedCrop(224),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.ColorJitter(brightness=0.2, contrast=0.2),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    # éªŒè¯æ—¶ï¼šåªåšæ ‡å‡†åŒ–
    transform_val = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    # ===============  å…³é”®ä¿®æ”¹ï¼šåŠ è½½ä½ çš„æ•°æ®ç»“æ„ ===============
    # æ³¨æ„ï¼šImageFolder è¦æ±‚å­æ–‡ä»¶å¤¹åå°±æ˜¯ç±»åˆ«å
    full_dataset = datasets.ImageFolder(
        root=DATA_DIR,  # æ ¹ç›®å½•æ˜¯ "data"
        transform=None  # å…ˆä¸åº”ç”¨ transformï¼Œåé¢æ‰‹åŠ¨å¤„ç†
    )

    # æ£€æŸ¥ç±»åˆ«æ˜ å°„ï¼ˆç¡®ä¿ cats=0, dogs=1ï¼‰
    print(" Dataset classes:", full_dataset.classes)  # åº”è¯¥è¾“å‡º ['cats', 'dogs']
    print(" Total images:", len(full_dataset))

    # ===============  å…³é”®ä¿®æ”¹ï¼š80/20 è‡ªåŠ¨åˆ’åˆ† ===============
    # å›ºå®šéšæœºç§å­ç¡®ä¿å¯é‡å¤æ€§
    torch.manual_seed(42)
    np.random.seed(42)

    # è·å–æ‰€æœ‰ç´¢å¼•å¹¶æ‰“ä¹±
    indices = np.arange(len(full_dataset))
    np.random.shuffle(indices)

    # 80% è®­ç»ƒ, 20% éªŒè¯
    val_size = int(0.2 * len(full_dataset))
    train_indices, val_indices = indices[val_size:], indices[:val_size]

    # åˆ›å»ºè‡ªå®šä¹‰æ•°æ®é›†åº”ç”¨ä¸åŒ transform
    class CustomSubset(torch.utils.data.Dataset):
        def __init__(self, dataset, indices, transform=None):
            self.dataset = dataset
            self.indices = indices
            self.transform = transform

        def __len__(self):
            return len(self.indices)

        def __getitem__(self, idx):
            img, label = self.dataset[self.indices[idx]]
            if self.transform:
                img = self.transform(img)
            return img, label

    # åº”ç”¨ä¸åŒ transform
    train_dataset = CustomSubset(full_dataset, train_indices, transform_train)
    val_dataset = CustomSubset(full_dataset, val_indices, transform_val)

    # åˆ›å»º DataLoader
    train_loader = DataLoader(
        train_dataset,
        batch_size=BATCH_SIZE,
        shuffle=True,
        num_workers=0,  # ä¿®å¤ï¼šWindows é»˜è®¤è®¾ä¸º0ï¼Œé¿å…å¤šè¿›ç¨‹é—®é¢˜
        pin_memory=True  # GPU åŠ é€Ÿå…³é”®
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=BATCH_SIZE,
        shuffle=False,
        num_workers=0,  # ä¿®å¤ï¼šWindows é»˜è®¤è®¾ä¸º0
        pin_memory=True
    )

    print(f" Dataset split: {len(train_dataset)} train | {len(val_dataset)} val")
    print(f" Model will use: {DEVICE}")

    # =============== æ¨¡å‹å®šä¹‰ï¼ˆResNet18 é€‚é… PyTorch 2.5.1ï¼‰===============
    model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)  # è‡ªåŠ¨ä¸‹è½½é¢„è®­ç»ƒæƒé‡
    num_ftrs = model.fc.in_features
    model.fc = nn.Sequential(
        nn.Dropout(0.5),  # é˜²æ­¢è¿‡æ‹Ÿåˆ
        nn.Linear(num_ftrs, 2)  # 2 classes: cats(0), dogs(1)
    )
    model = model.to(DEVICE)

    # =============== è®­ç»ƒé…ç½® ===============
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer,
        mode='max',
        factor=0.5,
        patience=2,
        verbose=True  # ä¿®å¤ï¼šç§»é™¤è­¦å‘Šï¼ˆå·²å¤„ç†ï¼‰
    )

    best_val_acc = 0.0
    os.makedirs("checkpoints", exist_ok=True)

    # =============== è®­ç»ƒå¾ªç¯ ===============
    for epoch in range(NUM_EPOCHS):
        # ----- è®­ç»ƒé˜¶æ®µ -----
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0

        pbar = tqdm(train_loader, desc=f"Epoch {epoch + 1}/{NUM_EPOCHS}")
        for inputs, labels in pbar:
            inputs, labels = inputs.to(DEVICE, non_blocking=True), labels.to(DEVICE, non_blocking=True)

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            # è®¡ç®—å‡†ç¡®ç‡
            _, predicted = torch.max(outputs.data, 1)
            train_total += labels.size(0)
            train_correct += (predicted == labels).sum().item()
            train_loss += loss.item()

            pbar.set_postfix({
                "loss": f"{loss.item():.4f}",
                "acc": f"{100. * train_correct / train_total:.1f}%"
            })

        train_acc = 100. * train_correct / train_total

        # ----- éªŒè¯é˜¶æ®µ -----
        model.eval()
        val_correct = 0
        val_total = 0
        val_loss = 0.0

        with torch.no_grad():
            for inputs, labels in tqdm(val_loader, desc="  Validation", leave=False):
                inputs, labels = inputs.to(DEVICE, non_blocking=True), labels.to(DEVICE, non_blocking=True)
                outputs = model(inputs)
                loss = criterion(outputs, labels)

                _, predicted = torch.max(outputs.data, 1)
                val_total += labels.size(0)
                val_correct += (predicted == labels).sum().item()
                val_loss += loss.item()

        val_acc = 100. * val_correct / val_total
        avg_val_loss = val_loss / len(val_loader)

        # æ›´æ–°å­¦ä¹ ç‡ï¼ˆä½¿ç”¨æ–°APIï¼‰
        scheduler.step(val_acc)

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), "checkpoints/best.pth")
            print(f"ğŸ† New best model saved! Val Acc: {val_acc:.2f}%")

        print(f" Epoch {epoch + 1} | "
              f"Train Loss: {train_loss / len(train_loader):.4f} | "
              f"Train Acc: {train_acc:.2f}% | "
              f"Val Loss: {avg_val_loss:.4f} | "
              f"Val Acc: {val_acc:.2f}%")

    print(f"\n Training completed! Best validation accuracy: {best_val_acc:.2f}%")
    print(f" Final model saved to: checkpoints/best.pth")


if __name__ == '__main__':
    main()  # ä»…ä¸»è¿›ç¨‹æ‰§è¡Œ